{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Collection\n",
    "\n",
    "We are focusing on synopsis/summary (Overview) of the content to extract clues related to generes. In this section we are processing raw data and keeping only information we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#--> Read CSV file. \n",
    "df = pd.read_csv(\"./lite-dev-mlcontent_2018/features.csv.dev\", delimiter=\"|\", error_bad_lines=True)\n",
    "df2 = pd.read_csv(\"./lite-dev-mlcontent_2018/labels.csv.dev\",  delimiter=\"|\", error_bad_lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9430,)\n",
      "(9430,)\n",
      "(9430, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#--> Projecting columns we are interesting in.\n",
    "data = df[['channel_name', 'program_name', 'keywords']]\n",
    "description = data[[\"channel_name\", 'program_name', 'keywords']].apply(lambda x: ' '.join(x), axis=1).values\n",
    "genres = df2.genre.map(lambda x: x.strip().lower()).str.split(\" \").values\n",
    "\n",
    "print(description.shape)\n",
    "print(genres.shape)\n",
    "\n",
    "lb = MultiLabelBinarizer()\n",
    "genres = lb.fit_transform(genres)\n",
    "print(genres.shape)\n",
    "\n",
    "# #--> Dropping all rows where either genere or overview \n",
    "# data = data.dropna(subset=['overview', 'genres'])\n",
    "\n",
    "# #--> Encode all generes\n",
    "# genres =  pd.get_dummies(data['genres'].str.split(\"|\", expand=False).apply(pd.Series).stack()).sum(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fnchd Happening Now The president's deadline come:1,follow the daca issue:1,set a telephone hot line:1,resume the debate:1,White house secretary called:1,bring this thing:1,The court rulings nullify:1,expand the daca program:1,stop the program:1,Donald Trump working:1\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleanup and Preprocess\n",
    "We are using keras' ```Tokenizer``` and ```pad_sequence``` utility to encode our input to fixed size matrice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training Set X:(8958, 100), y:(8958, 64). Testing set X:(472, 100), y:(472, 64)\n"
     ]
    }
   ],
   "source": [
    "#--> Each sequence in the dataset\n",
    "seq_len = 100\n",
    "\n",
    "#--> Size of the common vacabulary \n",
    "max_vocab = 5000\n",
    "\n",
    "#--> Embeding dimension \n",
    "embedding_dim = 100\n",
    "\n",
    "#--> From columns to numpy array\n",
    "X_train = description\n",
    "y_train = genres\n",
    "\n",
    "#--> Tokenizing \n",
    "tokenzier = Tokenizer(num_words=max_vocab)\n",
    "tokenzier.fit_on_texts(X_train)\n",
    "\n",
    "#--> Word index NOTE: We need to use the same word index for testing as well\n",
    "idx = tokenzier.index_docs\n",
    "\n",
    "#--> Get the data vectorize\n",
    "X_train = tokenzier.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen=seq_len)\n",
    "\n",
    "#--> Spliting for \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=.05)\n",
    "\n",
    "print(\"[INFO] Training Set X:{}, y:{}. Testing set X:{}, y:{}\"\n",
    "      .format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Using GloVe.6B.100d. as the first embedding layer and experimenting with the following approaaches\n",
    "1. GloVe pre-trained layer(Frozen) with Dense classifier layer.\n",
    "2. GloVe pre-trained layer(Frozen) with one or two convolution layers on tope.\n",
    "3. Fine tune embedding to see if there's any improvement.\n",
    "4. Create our own embedding layer and train it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 400000 word vectors.\n",
      "[INFO] Found 22191 elements in matrix.\n"
     ]
    }
   ],
   "source": [
    "# This particular block of code is from keras' official blog.\n",
    "# https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "#-- 1. Loading the indices\n",
    "f = open(os.path.join(\"./glove.6B\", 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "#-- 2. Creating embedding matrix\n",
    "embedding_matrix = np.zeros((len(idx) + 1, 50))\n",
    "\n",
    "for word, i in idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('[INFO] Found %s word vectors.' % len(embeddings_index))\n",
    "print('[INFO] Found %s elements in matrix.' % len(embedding_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 100, 50)           1109550   \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 98, 32)            4832      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 47, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 19, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 9, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 5, 32)             5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 64)                4160      \n",
      "=================================================================\n",
      "Total params: 1,131,950\n",
      "Trainable params: 1,131,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(idx)+1, \n",
    "                    output_dim=50,                     \n",
    "                    input_length=seq_len,\n",
    "                    weights = [embedding_matrix],\n",
    "                    trainable=True))\n",
    "\n",
    "model.add(Conv1D(32, 3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(32, 3, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(32, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Conv1D(32, 5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(2))\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dense(len(lb.classes_), activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 100, 64)           1421120   \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 64)                409664    \n",
      "=================================================================\n",
      "Total params: 1,830,784\n",
      "Trainable params: 1,830,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Embedding, GlobalMaxPooling1D, Dense, Flatten, MaxPooling1D, Dropout\n",
    "\n",
    "#print(X_train[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(idx)+1, output_dim=64, input_length=seq_len))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(128, activation=\"relu\"))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(len(lb.classes_), activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "```\n",
    ">>> from sklearn.model_selection import KFold\n",
    ">>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    ">>> y = np.array([1, 2, 3, 4])\n",
    ">>> kf = KFold(n_splits=2)\n",
    ">>> kf.get_n_splits(X)\n",
    "2\n",
    ">>> print(kf)  \n",
    "KFold(n_splits=2, random_state=None, shuffle=False)\n",
    ">>> for train_index, test_index in kf.split(X):\n",
    "...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "...    X_train, X_test = X[train_index], X[test_index]\n",
    "...    y_train, y_test = y[train_index], y[test_index]\n",
    "TRAIN: [2 3] TEST: [0 1]\n",
    "TRAIN: [0 1] TEST: [2 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8958 samples, validate on 2986 samples\n",
      "Epoch 1/3\n",
      "8958/8958 [==============================] - 8s 853us/step - loss: 0.0017 - acc: 0.9997 - val_loss: 6.5246e-04 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "8958/8958 [==============================] - 6s 714us/step - loss: 8.4069e-04 - acc: 0.9999 - val_loss: 4.5493e-04 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "8958/8958 [==============================] - 6s 719us/step - loss: 4.5949e-04 - acc: 1.0000 - val_loss: 3.1737e-04 - val_acc: 1.0000\n",
      "Train on 8958 samples, validate on 2986 samples\n",
      "Epoch 1/3\n",
      "8958/8958 [==============================] - 6s 715us/step - loss: 2.9589e-04 - acc: 1.0000 - val_loss: 1.6521e-04 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "8958/8958 [==============================] - 6s 714us/step - loss: 2.3706e-04 - acc: 1.0000 - val_loss: 1.1124e-04 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "8958/8958 [==============================] - 6s 719us/step - loss: 1.8499e-04 - acc: 1.0000 - val_loss: 8.5735e-05 - val_acc: 1.0000\n",
      "Train on 8958 samples, validate on 2986 samples\n",
      "Epoch 1/3\n",
      "8958/8958 [==============================] - 6s 711us/step - loss: 1.3016e-04 - acc: 1.0000 - val_loss: 8.0560e-05 - val_acc: 1.0000\n",
      "Epoch 2/3\n",
      "8958/8958 [==============================] - 7s 744us/step - loss: 1.5478e-04 - acc: 1.0000 - val_loss: 6.7225e-05 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "8958/8958 [==============================] - 6s 720us/step - loss: 9.1911e-05 - acc: 1.0000 - val_loss: 4.9218e-05 - val_acc: 1.0000\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`save_model` requires h5py.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-424139e8015c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#H = model.fit(X_train, y_train, epochs=epochs, validation_split=0.05, batch_size=32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2578\u001b[0m         \"\"\"\n\u001b[1;32m   2579\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2580\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`save_model` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_json_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: `save_model` requires h5py."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "epochs = 3\n",
    "file_name = \"./best_model.hdf5\"\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    batch_x, batch_y = X_train[train_index], y_train[train_index]\n",
    "    batch_xx, batch_yy = X_train[test_index], y_train[test_index]\n",
    "    H = model.fit(X_train, y_train, epochs=epochs, validation_data=(batch_xx, batch_yy))\n",
    "\n",
    "#H = model.fit(X_train, y_train, epochs=epochs, validation_split=0.05, batch_size=32)\n",
    "model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1280bd0f0>]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.arange(epochs), H.history[\"acc\"])\n",
    "plt.plot(np.arange(epochs), H.history[\"val_acc\"])\n",
    "plt.plot(np.arange(epochs), H.history[\"loss\"])\n",
    "plt.plot(np.arange(epochs), H.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate\n",
    "Here we need to show Accuracy, precision and may be F1 score. We will draw some charts to make sure overfitting and underfitting is not happening.\n",
    "\n",
    "We will use our train/test split data here to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472/472 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01830699016987267, 0.9954978813559322]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model evluation \n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "#model.score(X_test, y_test)\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Test and submit \n",
    "Generate the final test results, format and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3144, 100)\n"
     ]
    }
   ],
   "source": [
    "# Final test code goes here\n",
    "test_df = pd.read_csv(\"./lite-eval-mlcontent_2018/features.csv.eval\", delimiter=\"|\", error_bad_lines=True)\n",
    "test_data = test_df[['channel_name', 'program_name', 'keywords']]\n",
    "test_description = test_data[['program_name', 'keywords']].apply(lambda x: ' '.join(x), axis=1).values\n",
    "\n",
    "eval_data = tokenzier.texts_to_sequences(test_description)\n",
    "eval_data = pad_sequences(eval_data, maxlen=seq_len)\n",
    "\n",
    "print(eval_data.shape)\n",
    "\n",
    "predictions = model.predict(eval_data)\n",
    "\n",
    "header = \"|etag|\" + \"|\".join(lb.classes_)+\"|\\n\"\n",
    "\n",
    "output_file = \"./output/prediction.csv\"\n",
    "\n",
    "file = open(output_file, \"w\")\n",
    "# |etag| news | sports | ... |\n",
    "file.write(header)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    line = \"|\"+test_df[\"etag\"][i]+\"|\"+\"|\".join([\"{:.4f}\".format(a) for a in predictions[i]])+\"|\"\n",
    "    file.write(line)\n",
    "    file.write(\"\\n\")\n",
    "file.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
